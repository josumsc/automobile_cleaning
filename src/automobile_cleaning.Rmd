---
title: "Automobile Cleaning"
author: "Josu Alonso Castanedo & Emmanuel Rodríguez Belmonte"
date: "Diciembre 2020"
lang: "es-Es"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    highlight: tango

---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Descripción del Dataset

Al realizar la búsqueda de una dataset lo suficientemente potente como para realizar un proyecto de minería de datos completo sobre ésta, diversas fuentes vinieron a la cabeza, por lo que se realizó una exploración de los principales resultados de sitios web tales como [Kaggle](https://www.kaggle.com) o [UCI ML Repository](https://archive.ics.uci.edu), quedándonos al final con la dataset [Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile) de este último.

El porqué de dicha elección se basa en que esta dataset se presenta interesante para un proyecto de minería de datos debido a la presencia de valores nulos entre sus filas, lo que puede dar lugar a una limpieza más activa de los datos, así como al hecho de presentar una numerosa cantidad de variables, tanto categóricas como numéricas, entre las que destacamos como categórica `make` (fabricante del automóvil) y `price` (precio de éste) por su correlación e importancia a la hora de discriminar un coche de otro. Esta gran cantidad de variables también hace de la dataset una candidata a procesos de reducción de dimensionalidad, lo que implicaría un ejercico aún mayor sobre ésta y un valor añadido para la práctica.

Una vez comentado el atractivo de la dataset, podemos bajar a objetivos más concretos del análisis. Así pues, podríamos decir que el objetivo principal de este documento es discernir qué factores diferencian a los principales proveedores de vehículos automovilísticos, así como la relación entre la pertenencia a uno de estos grupos y el precio del automóvil en cuestión.

# Integración y selección de los datos de interés

Con los objetivos en mente y las principales características de la dataset descritas verbalmente, es el momento de bajar a la programación y ver datos más específicos y objetivos de nuestra dataset.

Para continuar con la práctica, no obstante, primero deberemos cargar los datos dentro de nuestro entorno para poder trabajar con ellos:

```{r message= FALSE, warning=FALSE}
# Importamos las librerías a utilizar durante el análisis
library(dplyr)
library(ggplot2)
library(reshape2)
library(corrgram)

# Establecemos una semilla por si queremos replicar el resultado
set.seed(555)


###### Carga del dataset ######

# Hardcodeamos el nombre de las columnas
dataset.names <- c(
  'symboling',
  'normalized-losses',
  'make',
  'fuel-type',
  'aspiration',
  'num-of-doors',
  'body-style',
  'drive-wheels',
  'engine-location',
  'wheel-base',
  'length',
  'width',
  'height',
  'curb-weight',
  'engine-type',
  'num-of-cylinders',
  'engine-size',
  'fuel-system',
  'bore',
  'stroke',
  'compression-ratio',
  'horsepower',
  'peak-rpm',
  'city-mpg',
  'highway-mpg',
  'price'
)

# Leemos nuestra versión del datase
dataset <- read.csv('../data/imports-85-OLD.data',
                    header = F,
                    col.names = dataset.names,
                    # En este dataset los NA están representados como '?'
                    na.strings = c('?','', ' '))

# Convertimos las columnas con variables categóricas a factores

factores <- c("make",
              "fuel.type",
              "aspiration",
              "num.of.doors",
              "body.style",
              "engine.type",
              "num.of.cylinders",
              "fuel.system",
              "drive.wheels",
              "engine.location")

for(f in factores){
  dataset[, f] <- as.factor(dataset[, f])
}
```


Una vez realizada la carga de nuestros datos, podemos pasar a realizar una serie de descripciones básicas:

```{r message= FALSE, warning=FALSE}
###### Descripción del dataset ######

# Describimos la estructura de las variables
str(dataset)

# La distribución de sus atributos
summary(dataset)

# Creamos una versión "melted" de nuestra dataset para visualizar más fácilmente
melt.dataset <- melt(dataset)

# Visualizamos la distribución de los atributos continuos de nuestra dataset
ggplot(data = melt.dataset, aes(x = value)) + 
  stat_density() + 
  facet_wrap(~variable, scales = "free")
```

Como podemos observar, nuestra dataset está formada por **205 observaciones y 26 variables**, de las cuáles **16 son numéricas y 10 son categóricas**. también observamos que la mayoría de observaciones pertenecen a la clase `gas` de la variable `fuel.type`, así como que la mayoría también se encuentran dentro del subgrupo `std` de la variable `aspiration`.

Por lo que corresponde a las variables objetivo de nuestro análisis, vemos que la distribución de `make` es estable, ya que a pesar de tener el primer fabricante (*toyota*) 32 observaciones (15,6% del total de las observaciones) el resto del top 6 de facbricantes se mantiene en el rango 18-12, por lo que podríamos coger estos 6 fabricantes como subgrupos en los cuales dividir las observaciones y aún así mantener una muestra bastante equitativa de los datos. El precio por contra, mantinene un rango **5118 - 45400**, lo que indica que hay observaciones por bastante por encima del 3º cuartil (16500), pero no muy por debajo del primero (7775).

El resto de variables presentan una distribución bastante normal, a destacar el hecho de que `compression.ratio` presenta claros valores extremos alrededor de su máximo, como podemos ver en la gráfica de su distribución. Por su parte, `horsepower`, `engine.size` y `normalized.losses` presentan una distribución similar a `price`, alejándose de la normalidad, lo que podría indicar una correlación entre estas variables.

Si visualizamos las variables categóricas que más sesgan la dataset (`fuel.type`,`aspiration` y `make`), ya que así podremos obtener un poco más de información sobre éstas:

```{r message= FALSE, warning=FALSE}
# Visualizamos la distribución de un par de factores
barplot(with(dataset, table(fuel.type)))        # fuel.type
barplot(with(dataset, table(aspiration)))       # aspiration
barplot(with(dataset, table(make)))             # make
```


En lo que respecta a la correlación entre las variables, también podemos realizar visualizaciones:

```{r message= FALSE, warning=FALSE}
# Visualizamos la correlación de estos atributos
corrgram(dataset,
         order = T,
         lower.panel = panel.shade,
         text.panel = panel.txt)
```

Y comprobar que hay grandes correlaciones entre `price` y `engine.size`, como habíamos anunciado, así como entre otras variables, como `city.mpg` y `highway.mpg` (ya que si un automóvil tiene un gran consumo en ciudad también lo tendrá en autopista). A su vez, vemos como estas dos últimas variables afectan negativamente al precio, lo que también tiene sentido.

# Limpieza de los datos

Una vez seleccionados los datos, hay una serie de pasos que tenemos que realizar para asegurar una cierta limpieza de éstos. Entre los pasos a realizar, los más importantes son el tratamiento de valores nulos, los cuáles ya hemos identificado y asignado el valor **NA** en la carga de datos, y el de los valores extremos, también identificados dentro de la variable `compression.ratio`.

## Elementos vacíos

Primero, debemos comprobar qué columnas presentan valores vacíos y en qué proporción de su total para decidir qué estrategia tomar ante su presencia en nuestros datos. Para eso, veremos sus valores absolutos y relativos mediante la función `colSums`:

```{r message= FALSE, warning=FALSE}
# Buscamos valores nulos 
colSums(is.na(dataset))

# Representamos su peso sobre el total de las observaciones
colSums(is.na(dataset))/nrow(dataset)
```


```{r message= FALSE, warning=FALSE}
# Eliminamos observaciones con valor "?" en algunos atributos críticos
dataset <- dataset[(!is.na(dataset$horsepower)) & (!is.na(dataset$num.of.doors)) & (!is.na(dataset$price)), ]

# E imputamos la media para los siguientes
for (column in c('normalized.losses', 'bore', 'stroke')){
  dataset[is.na(dataset[, column]), column] <- mean(dataset[, column], na.rm = T)
}
```

## Identificación y tratamiento de valores extremos

Al describir la dataset nos percatamos de la presencia de valores extremos en la variable `compression.ratio` alrededor de su valor máximo. Pero, a pesar de parecer clara su presencia observando el histograma de la variable, podemos asegurarnos todavía más de su presencia mediante la realización de **boxplots** que nos muestren la relación de estos valores con sus cuartiles, y cómo de extremos son dichos valores:

```{r message= FALSE, warning=FALSE}
# Recordamos la distribución de compression.ratio
summary(dataset$compression.ratio)

# Realizamos nuestro plot
boxplot(dataset$compression.ratio)
```

Como podemos observar, estos valores están por encima de **20**, cuando el tercer cuartil de la variable es **9,40**, por lo que suponen una desviación de más de **4,58 veces el rango intercuartílico** desde dicho cuartil, por lo que podemos confirmar su estatus de valores extremos.

Una vez confirmada la presencia de valores extremos en esta variable, tenemos que decidir qué estrategia seguir para su tratamiento. Teniendo en cuenta su lejanía del 3 cuartil, así como su escaso número, creemos que el tratamiento por eliminación es el más acertado, ya que la cantidad de información a perder es mínima y el riesgo de imputar un valor medio sin estar seguros de un correcto registro del resto de variables es alto. Por lo tanto, procedemos a la eliminación de estas observaciones de nuestra dataset:

```{r message= FALSE, warning=FALSE}
# Eliminamos los datos que corresponden a dicho outlier
dataset = dataset[!dataset$compression.ratio >= 20, ]
```

# Análisis de los datos

Una vez hemos observado las variables en general, vamos a realizar un estudio estadístico para sacar conclusiones con respecto a los atributos que consideremos importantes al inicio del documento (precio y marca).

## Selección de los grupos

Vamos a estudiar cómo afectan las marcas según continente en los distintos parámetros, haciendo especial énfasis en el precio.

Como vamos a poner especial atención en el precio, crearemos un nuevo atributo price.range, para poder realizar análisis categóricos con el precio, tomando como referencia los siguientes rangos:

- low: Precio entre 0 y el primer cuartil (7775).
- medium: Precio entre el primer y tercer cuartil (7775 y 16500).
- high: Precio mayor al tercer cuartil (16500).


```{r message= FALSE, warning=FALSE}
nrow(dataset)

# Creación del nuevo campo price.range para los rangos definidos
for (i in 1:nrow(dataset)) {
  if (dataset[i, 'price'] <= 7775){
    dataset[i, 'price.range'] = 'low'
  } else if(dataset[i, 'price'] <= 16500){
    dataset[i, 'price.range'] = 'medium'
  } else{
    dataset[i, 'price.range'] = 'high'
  }
}

# Guardamos como factor el nuevo atributo
dataset[,'price.range'] = as.factor(dataset[,'price.range'])
```

Podemos ver cómo queda el nuevo atributo categórico en la siguiente tabla:
```{r message= FALSE, warning=FALSE}
# Visualizamos la distribución de price.range
barplot(with(dataset, table(price.range)))
```

Por último, vamos a crear **n** grupos, uno por cada proveedor, para poder ver cómo la marca según continente afecta a los distintos atributos (y principalmente al precio), por lo que realizaremos tres tipos de análisis:

- Correlación de los atributos con el precio por marca según continente, comparando con la general.
- Predicción del precio con regresión lineal por marca según continente y en general, comparando los resultados obtenidos entre marcas según continente y con el general.
- Predicción del precio con regresión logística por marca según continente y en general, comparando los resultados obtenidos entre marcas según continente y con el general.

```{r message= FALSE, warning=FALSE}
maker_imputer <- function(maker){
  if(maker %in% c('alfa-romero', 'audi', 'bmw', 'mercedes-benz', 'volvo',
                  'peugot', 'porsche', 'saab', 'volkswagen', 'jaguar')){
    x <- 'europe'
  } else if (maker %in% c('chevrolet', 'dodge', 'mercury', 'plymouth')){
    x <- 'america'
  } else {
    x <- 'asia'
  }
  return(x)
}

dataset$make.continent <- sapply(dataset$make, maker_imputer)
barplot(with(dataset, table(make.continent)))

# Separamos el dataset por marcas
continents <- c('america', 'asia', 'europe')

dataset_by_continent <- list()

for (continent in continents) {
  dataset_by_continent[continent] = 
    list(dataset[dataset$make.continent == continent, ])
}
```

## Comprobación de la normalidad y homogeneidad de la varianza

Vamos a realizar la comprobación de la normalidad de todos los datos y de la homogeneidad de la varianza utilizando el teorema del límite central y análisis de la varianza (ANOVA) respectivamente.

La razón de esto, es que no tenemos necesidad de saber la normalidad u homogeneidad por los grupos, ya que no son condición para ninguno de los anális que vamos a realizar.

El teorema del límite central, de forma resumida, nos permite asumir la normalidad de los datos cuando el número de muestras supera un cierto número (30).

Por tanto, vamos a comprobar el número de observaciones de las que disponemos tras la limpieza de los datos, para confirmar que tenemos más de 30.

```{r message= FALSE, warning=FALSE}
# Mostramos el número de elementos tras la limpieza
nrow(dataset)
```

Tras la limpieza, el número de observaciones es 178, por lo que gracias al teorema del límite central, podemos asumir la normalidad del conjunto de datos.

Para acabar con este punto, realizamos el análisis de la homocedasticidad, u homogeneidad de la varianza, sobre el dataset. Para poder realizar esta comprobación, usaremos el test de Barlett, ya que la diferencia en tamaño de los diferentes grupos a analizar es notable y este test nos asegura unos resultados comparables aún así siempre y cuando se cumpla el principio de normalidad (constatado en el punto anterior).

```{r message= FALSE, warning=FALSE}
# Recorremos cada una de las variables más interesantes del
# dataset y cada uno de los continentes
variables.to.test<- c('price', 'normalized.losses',
                      'horsepower','peak.rpm','engine.size')

for(variable in variables.to.test){
  asia <- dataset_by_continent[['asia']][,variable]
  europe <- dataset_by_continent[['europe']][,variable]
  america <- dataset_by_continent[['america']][,variable]
  print(bartlett.test(list(asia, europe, america)))
}

```

Como podemos comprobar, el resultado obtenido muestra que con un p-value aceptable para `horsepower` y `normalized.losses` aceptamos la hipótesis nula, que nos indica que las varianzas son iguales. En cambio, para las otras 3 variables, vemos unos valores de p-value bastante bajos, que indican que la varianza de los atributos es diferente en los grupos elegidos.

## Aplicación de las pruebas estadísticas


Para la primera prueba, la comparación de la correlación entre los atributos y el precio por marca (según continente) y de forma general, por lo que realizamos los cálculos y los guardamos para visualizarla y sacar conclusiones de ello en los siguientes apartados.

```{r message= FALSE, warning=FALSE}
# Calculamos las correlaciones
cuantitative_fields <- c('symboling', 'normalized.losses', 'wheel.base', 'length', 'width', 
                         'height', 'curb.weight', 'engine.size', 'bore', 'stroke', 
                         'compression.ratio', 'horsepower', 'peak.rpm', 'city.mpg', 'highway.mpg')

corr_by_continent <- list()

for (continent in continents) {
 for (field in cuantitative_fields) {
  corr_by_continent[[continent]][field] = cor(dataset_by_continent[[continent]][,field], dataset_by_continent[[continent]][,'price'])
 }
}

general_correlation <- list()
for (field in cuantitative_fields) {
  general_correlation[field] = cor(dataset[,field], dataset$price)
}
```

Lo siguiente es hacer un modelo de regresión lineal para cada uno de los continentes y el general que intente entender la causística detrás de las variaciones en el precio:

```{r message= FALSE, warning=FALSE}
# Creación de los modelos de regresión lineal por continente
lm_asia <- lm (formula = price.range ~ symboling +
                 normalized.losses + wheel.base + length +
                 width + height + curb.weight + engine.size +
                 bore + stroke + compression.ratio + horsepower + 
                 peak.rpm + city.mpg + highway.mpg, 
               data = dataset_by_continent[['asia']])

lm_america <- lm (formula = price.range ~ symboling + 
                    normalized.losses + wheel.base + length + 
                    width + height + curb.weight + engine.size +
                    bore + stroke + compression.ratio + horsepower + 
                    peak.rpm + city.mpg + highway.mpg, 
                  data = dataset_by_continent[['america']])

lm_europe <- lm (formula = price.range ~ symboling + 
                   normalized.losses + wheel.base + length + 
                   width + height + curb.weight + engine.size + 
                   bore + stroke + compression.ratio + horsepower +
                   peak.rpm + city.mpg + highway.mpg, 
                 data = dataset_by_continent[['europe']])

lm_general <- lm (formula = price.range ~ symboling + 
                    normalized.losses + wheel.base + length +
                    width + height + curb.weight + engine.size + 
                    bore + stroke + compression.ratio + horsepower +
                    peak.rpm + city.mpg + highway.mpg, data = dataset)

```

Por último, los modelos de regresión logística:

```{r message= FALSE, warning=FALSE}
# Creación de los modelos de regresión logística por continente
glm_asia <- glm (formula = price.range ~ symboling + 
                   normalized.losses + wheel.base + length + 
                   width + height + curb.weight + engine.size + 
                   bore + stroke + compression.ratio + horsepower + 
                   peak.rpm + city.mpg + highway.mpg, 
                 data = dataset_by_continent[['asia']],
                 family=binomial)

glm_america <- glm (formula = price.range ~ symboling + 
                      normalized.losses + wheel.base + length + 
                      width + height + curb.weight + engine.size + 
                      bore + stroke + compression.ratio + horsepower + 
                      peak.rpm + city.mpg + highway.mpg, 
                    data = dataset_by_continent[['america']],
                    family=binomial)

glm_europe <- glm (formula = price.range ~ symboling + 
                     normalized.losses + wheel.base + length +
                     width + height + curb.weight + engine.size + 
                     bore + stroke + compression.ratio + horsepower + 
                     peak.rpm + city.mpg + highway.mpg, 
                   data = dataset_by_continent[['europe']],
                   family=binomial)

glm_general <- glm (formula = price.range ~ symboling + 
                    normalized.losses + wheel.base + length +
                    width + height + curb.weight + engine.size + 
                    bore + stroke + compression.ratio + horsepower +
                    peak.rpm + city.mpg + highway.mpg, 
                    data = dataset, family=binomial)
```

# Representación de los resultados

Una vez resueltas las pruebas estadísticas, podemos visualizar las variables de cada grupo para entender mejor sus diferencias y sus puntos comunes, así como el efecto de cada variable sobre su precio en cada uno de los grupos.

```{r message= FALSE, warning=FALSE}
# Mostramos la correlación de los atributos numéricos de cada continente
for (continent in continents){
  corrgram(dataset_by_continent[[continent]],
         order = T,
         lower.panel = panel.shade,
         text.panel = panel.txt)
}
```

Como podemos comprobar, los elementos de los automóviles europeos tienen correlaciones menos fuertes entre ellos, llegando a ver colores mucho más apagados en el gráfico perteneciente a este continente. Además, incluso nuestra variable original `price` recibe una influencia por el resto de sus compañeras mucho más pequeña que en el resto de continentes, lo que podría indicar una predisposición hacia un modelo de coche más estándar y menos guiado por variables extrapoladas que empujasen al resto a variar en mayor cuantía.

Continuando con un raciocinio similar, observamos que los fabricantes americanos tienden a incorporar una mayor variabilidad en los atributos de sus coches, llegando a mostrar coeficientes de corelación mucho más altos. Vemos en especial que la variable precio es fuertemente influida por muchas de las variables, dejando `height` fuera de esta ecuación.

```{r message= FALSE, warning=FALSE}
# Mostramos la distribución de precio de cada continente
for (continent in continents){
  hist(dataset_by_continent[[continent]][,'price'])
}

# Y la comparamos con sus quartiles con los boxplots
for (continent in continents){
  boxplot(dataset_by_continent[[continent]][,'price'])
}
```


En lo relativo al precio, observamos que los coches europeos son los que tienen un rango de precio mayor, llegando sus precios hasta superar los 500.000, mientras que en Asia y América llegaríamos a 200.000 y 180.000 respectivamente para el último bin de su histograma. Además, podemos ver también una predisposición de los coches europeos a la polaridad, al observar 2 grupos claros entorno a los precios bajos y altos del histograma, cosa que también observamos en menor medida en los otros 2 continentes a los que pertenecen los fabricantes, aunque si bien es cierto que en estos otros 2 grupos la evolución de la distribución es más gradual y menos diferenciada en grupos.

En cuanto a valores extremos, observamos que América es la cual presenta valores más alejados de su 3º cuartil posicionándose entre 130.000 y 160.000, llegando a ser considerados estos valores como extremos per se. En los otros 2 grupos, los valores cercanos al máximo si bien superan el 3º cuartil por una buena diferencia, no sería claro el considerarlos extremos al estar a una distancia todavía prudencial.


# Resolución del problema

